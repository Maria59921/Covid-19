{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHxWx9ikjeD7",
        "outputId": "a83a6cf3-d113-4581-ab2c-00c5b2400689"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting seclea-ai==1.0.2\n",
            "  Downloading seclea_ai-1.0.2-py3-none-any.whl (281 kB)\n",
            "\u001b[K     || 281 kB 8.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt in /usr/local/lib/python3.7/dist-packages (from seclea-ai==1.0.2) (1.14.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from seclea-ai==1.0.2) (4.4.2)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from seclea-ai==1.0.2) (1.3.5)\n",
            "Requirement already satisfied: dill>=0.3.4 in /usr/local/lib/python3.7/dist-packages (from seclea-ai==1.0.2) (0.3.6)\n",
            "Collecting pickleDB>=0.9.2\n",
            "  Downloading pickleDB-0.9.2.tar.gz (3.7 kB)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from seclea-ai==1.0.2) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.3.0->seclea-ai==1.0.2) (1.21.6)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.3.0->seclea-ai==1.0.2) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.3.0->seclea-ai==1.0.2) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.3.0->seclea-ai==1.0.2) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->seclea-ai==1.0.2) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->seclea-ai==1.0.2) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->seclea-ai==1.0.2) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->seclea-ai==1.0.2) (3.0.4)\n",
            "Building wheels for collected packages: pickleDB\n",
            "  Building wheel for pickleDB (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pickleDB: filename=pickleDB-0.9.2-py3-none-any.whl size=4271 sha256=6e61f58e58b11f541e3a44d6aeb1921de0d690094b6b7d67fbb3ce4f9b52b734\n",
            "  Stored in directory: /root/.cache/pip/wheels/08/34/42/9a7f94099208ce3d32638d98586a5a50f821db2fc75a3bdaae\n",
            "Successfully built pickleDB\n",
            "Installing collected packages: pickleDB, seclea-ai\n",
            "Successfully installed pickleDB-0.9.2 seclea-ai-1.0.2\n",
            "/bin/bash: conda: command not found\n"
          ]
        }
      ],
      "source": [
        "!pip install seclea-ai==1.0.2\n",
        "!conda install seclea-ai==1.0.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from seclea_ai import SecleaAI\n",
        "\n",
        "# NOTE - use the organization name provided to you when issued credentials.\n",
        "seclea = SecleaAI(project_name=\"Covid-19 detection\", organization='Seclea')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zu4MDXF0kWMb",
        "outputId": "21f30943-8848-493b-f9a0-b4b5f54957b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Username: mariaantony\n",
            "Password: 路路路路路路路路路路\n",
            "success\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# load the data\n",
        "data = pd.read_csv('/content/Covid Dataset.csv')\n",
        "\n",
        "\n",
        "# define the metadata for the dataset.\n",
        "dataset_metadata = {\"outcome_name\": \"COVID-19\",\n",
        "                    \"favourable_outcome\": \"N\",\n",
        "                    \"unfavourable_outcome\": \"Y\",\n",
        "                    \"continuous_features\": [\n",
        "                                            \"Breathing Problem\",\n",
        "                                            'Fever',\n",
        "                                            'Dry Cough',\n",
        "                                            'Sore throat',\n",
        "                                            'Running Nose',\n",
        "                                            'Asthma',\n",
        "                                            'Chronic Lung Disease',\n",
        "                                            'Headache',\n",
        "                                            'Heart Disease',\n",
        "                                            'Diabetes',\n",
        "                                            'Hyper Tension',\n",
        "                                            'Fatigue',\n",
        "                                            'Gastrointestinal',\n",
        "                                            'Abroad travel',\n",
        "                                            'Contact with COVID Patient',\n",
        "                                            'Attended Large Gathering',\n",
        "                                            'Visited Public Exposed Places',\n",
        "                                            'Family working in Public Exposed Places',\n",
        "                                            'Wearing Masks',\n",
        "                                            'Sanitization from Market',\n",
        "\n",
        "\n",
        "                                            ]}\n",
        "\n"
      ],
      "metadata": {
        "id": "fCmMEggZkmn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a copy to isolate the original dataset\n",
        "df1 = data.copy(deep=True)\n",
        "\n",
        "def encode_nans(df):\n",
        "    # convert the special characters to nans\n",
        "    return df.replace('?', np.NaN)\n",
        "\n",
        "df2 = encode_nans(df1)"
      ],
      "metadata": {
        "id": "fAG1hTCSo36a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Drop the the column which are more than some proportion NaN values\n",
        "def drop_nulls(df, threshold):\n",
        "    cols = [x for x in df.columns if df[x].isnull().sum() / df.shape[0] > threshold]\n",
        "    return df.drop(columns=cols)\n",
        "\n",
        "# We choose 95% as our threshold\n",
        "null_thresh = 0.95\n",
        "df3 = drop_nulls(df2, threshold=null_thresh)\n",
        "\n",
        "def drop_correlated(data, thresh):\n",
        "    import numpy as np\n",
        "\n",
        "    # calculate correlations\n",
        "    corr_matrix = data.corr().abs()\n",
        "    # get the upper part of correlation matrix\n",
        "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
        "\n",
        "    # columns with correlation above threshold\n",
        "    redundant = [column for column in upper.columns if any(upper[column] >= thresh)]\n",
        "    print(f\"Columns to drop with correlation > {thresh}: {redundant}\")\n",
        "    new_data = data.drop(columns=redundant)\n",
        "    return new_data\n",
        "\n",
        "# drop columns that are too closely correlated\n",
        "correlation_threshold = 0.95\n",
        "df4 = drop_correlated(df3, correlation_threshold)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aQdm2Kdo_9o",
        "outputId": "405d2c0a-8a69-462d-e057-1a1ad1f4d406"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns to drop with correlation > 0.95: []\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  app.launch_new_instance()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from seclea_ai.transformations import DatasetTransformation\n",
        "\n",
        "\n",
        "# define the updates to the metadata - only changes are updated - here a continuous feature has been dropped so now\n",
        "# we remove it from the list of continuous features.\n",
        "processed_metadata = {\"continuous_features\": [\n",
        "                                           \"Breathing Problem\",\n",
        "                                            'Fever',\n",
        "                                            'Dry Cough',\n",
        "                                            'Sore throat',\n",
        "                                            'Running Nose',\n",
        "                                            'Asthma',\n",
        "                                            'Chronic Lung Disease',\n",
        "                                            'Headache',\n",
        "                                            'Heart Disease',\n",
        "                                            'Diabetes',\n",
        "                                            'Hyper Tension',\n",
        "                                            'Fatigue',\n",
        "                                            'Gastrointestinal',\n",
        "                                            'Abroad travel',\n",
        "                                            'Contact with COVID Patient',\n",
        "                                            'Attended Large Gathering',\n",
        "                                            'Visited Public Exposed Places',\n",
        "                                            'Family working in Public Exposed Places',\n",
        "                                            'Wearing Masks',\n",
        "                                            'Sanitization from Market',\n",
        "                                            ]}\n",
        "\n",
        "#  define the transformations - note the arguments\n",
        "cleaning_transformations = [\n",
        "            DatasetTransformation(encode_nans, data_kwargs={\"df\": df1}, kwargs={}, outputs=[\"df\"]),\n",
        "            DatasetTransformation(\n",
        "                drop_nulls, data_kwargs={\"df\": \"inherit\"}, kwargs={\"threshold\": null_thresh}, outputs=[\"data\"]\n",
        "            ),\n",
        "            DatasetTransformation(\n",
        "                drop_correlated, data_kwargs={\"data\": \"inherit\"}, kwargs={\"thresh\": correlation_threshold}, outputs=[\"df\"]\n",
        "            ),\n",
        "        ]\n",
        "\n",
        "\n",
        "\n",
        "def fill_nan_const(df, val):\n",
        "    \"\"\"Fill NaN values in the dataframe with a constant value\"\"\"\n",
        "    return df.replace(['None', np.nan], val)\n",
        "\n",
        "\n",
        "# Fill nans in 1st dataset with -1\n",
        "const_val = -1\n",
        "df_const = fill_nan_const(df4, const_val)\n",
        "\n",
        "def fill_nan_mode(df, columns):\n",
        "    \"\"\"\n",
        "    Fills nans in specified columns with the mode of that column\n",
        "    Note that we want to make sure to not modify the dataset we passed in but to\n",
        "    return a new copy.\n",
        "    We do that by making a copy and specifying deep=True.\n",
        "    \"\"\"\n",
        "    new_df = df.copy(deep=True)\n",
        "    for col in df.columns:\n",
        "        if col in columns:\n",
        "            new_df[col] = df[col].fillna(df[col].mode()[0])\n",
        "    return new_df\n",
        "\n",
        "nan_cols = ['Headache','Fatigue', 'Asthma']\n",
        "df_mode = fill_nan_mode(df4, nan_cols)\n",
        "\n",
        "\n",
        "\n",
        "# find columns with categorical data for both dataset\n",
        "cat_cols = df_const.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "def encode_categorical(df, cat_cols):\n",
        "  from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "  new_df = df.copy(deep=True)\n",
        "  for col in cat_cols:\n",
        "    if col in df.columns:\n",
        "        le = LabelEncoder()\n",
        "        le.fit(list(df[col].astype(str).values))\n",
        "        new_df[col] = le.transform(list(df[col].astype(str).values))\n",
        "  return new_df\n",
        "\n",
        "df_const = encode_categorical(df_const, cat_cols)\n",
        "df_mode = encode_categorical(df_mode, cat_cols)\n",
        "\n",
        "# Update metadata with new encoded values for the outcome column.\n",
        "encoded_metadata = {\"favourable_outcome\": 0,\n",
        "                    \"unfavourable_outcome\": 1,}\n",
        "\n",
        "\n",
        "#  define the transformations - for the constant fill dataset\n",
        "const_processed_transformations = [\n",
        "    DatasetTransformation(fill_nan_const, data_kwargs={\"df\": df4}, kwargs={\"val\": const_val}, outputs=[\"df\"]),\n",
        "    DatasetTransformation(encode_categorical, data_kwargs={\"df\": \"inherit\"}, kwargs={\"cat_cols\":cat_cols}, outputs=[\"df\"]),\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "#  define the transformations - for the mode fill dataset\n",
        "mode_processed_transformations = [\n",
        "    DatasetTransformation(fill_nan_mode, data_kwargs={\"df\": df4}, kwargs={\"columns\": nan_cols}, outputs=[\"df\"]),\n",
        "    DatasetTransformation(encode_categorical, data_kwargs={\"df\": \"inherit\"}, kwargs={\"cat_cols\": cat_cols}, outputs=[\"df\"]),\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "def get_samples_labels(df, output_col):\n",
        "    X = df.drop(output_col, axis=1)\n",
        "    y = df[output_col]\n",
        "\n",
        "    return X, y\n",
        "\n",
        "# split the datasets into samples and labels ready for modelling.\n",
        "X_const, y_const = get_samples_labels(df_const, \"COVID-19\")\n",
        "X_mode, y_mode = get_samples_labels(df_mode, \"COVID-19\")\n",
        "\n",
        "def get_test_train_splits(X, y, test_size, random_state):\n",
        "    from sklearn.model_selection import train_test_split\n",
        "\n",
        "    return train_test_split(\n",
        "        X, y, test_size=test_size, stratify=y, random_state=random_state\n",
        "    )\n",
        "    # returns X_train, X_test, y_train, y_test\n",
        "\n",
        "# split into test and train sets\n",
        "X_train_const, X_test_const, y_train_const, y_test_const = get_test_train_splits(X_const, y_const, test_size=0.2, random_state=42)\n",
        "X_train_mode, X_test_mode, y_train_mode, y_test_mode = get_test_train_splits(X_mode, y_mode, test_size=0.2, random_state=42)\n",
        "\n",
        "#  define the transformations - for the constant fill training set\n",
        "const_train_transformations = [\n",
        "    DatasetTransformation(\n",
        "            get_test_train_splits,\n",
        "            data_kwargs={\"X\": X_const, \"y\": y_const},\n",
        "            kwargs={\"test_size\": 0.2, \"random_state\": 42},\n",
        "            outputs=[\"X_train_const\", None, \"y_train_const\", None],\n",
        "            split=\"train\",\n",
        "            ),\n",
        "]\n",
        "\n",
        "# 猬锔 upload the const fill training set\n",
        "#seclea.upload_dataset_split(\n",
        " #                       X=X_train_const,\n",
        "  #                      y=y_train_const,\n",
        "   #                     dataset_name=\"Heart failure - Const Fill - Train\",\n",
        "    #                    metadata={},\n",
        "     #                   transformations=const_train_transformations\n",
        "#)\n",
        "\n",
        "#  define the transformations - for the constant fill test set\n",
        "const_test_transformations = [\n",
        "    DatasetTransformation(\n",
        "            get_test_train_splits,\n",
        "            data_kwargs={\"X\": X_const, \"y\": y_const},\n",
        "            kwargs={\"test_size\": 0.2, \"random_state\": 42},\n",
        "            outputs=[None, \"X_test_const\", None, \"y_test_const\"],\n",
        "            split=\"test\"\n",
        "            ),\n",
        "]\n",
        "\n",
        "# 猬锔 upload the const fill test set\n",
        "#seclea.upload_dataset_split(X=X_test_const,\n",
        " #                    y=y_test_const,\n",
        "  #                    dataset_name=\"Heart failure - Const Fill - Test\",\n",
        "   #                   metadata={},\n",
        "    #                  transformations=const_test_transformations)\n",
        "\n",
        "#  define the transformations - for the mode fill training set\n",
        "mode_train_transformations = [\n",
        "    DatasetTransformation(\n",
        "            get_test_train_splits,\n",
        "            data_kwargs={\"X\": X_mode, \"y\": y_mode},\n",
        "            kwargs={\"test_size\": 0.2, \"random_state\": 42},\n",
        "            outputs=[\"X_train_mode\", None, \"y_train_mode\", None],\n",
        "            split=\"train\",\n",
        "            ),\n",
        "]\n",
        "\n",
        "# 猬锔 upload the mode fill train set\n",
        "#seclea.upload_dataset_split(X=X_train_mode,\n",
        " #                     y=y_train_mode,\n",
        "  #                    dataset_name=\"Heart failure - Mode Fill - Train\",\n",
        "   #                   metadata=processed_metadata,\n",
        "    #                  transformations=mode_train_transformations)\n",
        "\n",
        "#  define the transformations - for the mode fill test set\n",
        "mode_test_transformations = [\n",
        "    DatasetTransformation(\n",
        "            get_test_train_splits,\n",
        "            data_kwargs={\"X\": X_mode, \"y\": y_mode},\n",
        "            kwargs={\"test_size\": 0.2, \"random_state\": 42},\n",
        "            outputs=[None, \"X_test_mode\", None, \"y_test_mode\"],\n",
        "            split=\"test\",\n",
        "            ),\n",
        "]\n",
        "\n",
        "# 猬锔 upload the mode fill test set\n",
        "#seclea.upload_dataset_split(X=X_test_mode,\n",
        " #                    y=y_test_mode,\n",
        "  #                   dataset_name=\"Heart failure - Mode Fill - Test\",\n",
        "   #                  metadata={},\n",
        "    #                transformations=mode_test_transformations)\n",
        "#\n",
        "\n",
        "\n",
        "def smote_balance(X, y, random_state):\n",
        "    from imblearn.over_sampling import SMOTE\n",
        "\n",
        "    sm = SMOTE(random_state=random_state)\n",
        "\n",
        "    X_sm, y_sm = sm.fit_resample(X, y)\n",
        "\n",
        "    print(\n",
        "        f\"\"\"Shape of X before SMOTE: {X.shape}\n",
        "    Shape of X after SMOTE: {X_sm.shape}\"\"\"\n",
        "    )\n",
        "    print(\n",
        "        f\"\"\"Shape of y before SMOTE: {y.shape}\n",
        "    Shape of y after SMOTE: {y_sm.shape}\"\"\"\n",
        "    )\n",
        "    return X_sm, y_sm\n",
        "    # returns X, y\n",
        "\n",
        "# balance the training sets - creating new training sets for comparison\n",
        "X_train_const_smote, y_train_const_smote = smote_balance(X_train_const, y_train_const, random_state=42)\n",
        "X_train_mode_smote, y_train_mode_smote = smote_balance(X_train_mode, y_train_mode, random_state=42)\n",
        "\n",
        "#  define the transformations - for the constant fill balanced train set\n",
        "const_smote_transformations = [\n",
        "    DatasetTransformation(\n",
        "            smote_balance,\n",
        "            data_kwargs={\"X\": X_train_const, \"y\": y_train_const},\n",
        "            kwargs={\"random_state\": 42},\n",
        "            outputs=[\"X\", \"y\"]\n",
        "            ),\n",
        "]\n",
        "\n",
        "# 猬锔 upload the constant fill balanced train set\n",
        "#seclea.upload_dataset_split(X=X_train_const_smote,\n",
        " #                     y=y_train_const_smote,\n",
        "  #                    dataset_name=\"Heart failure - Const Fill - Smote Train\",\n",
        "   #                   metadata={},\n",
        "    #                  transformations=const_smote_transformations)\n",
        "\n",
        "#  define the transformations - for the mode fill balanced train set\n",
        "mode_smote_transformations = [\n",
        "    DatasetTransformation(\n",
        "            smote_balance,\n",
        "            data_kwargs={\"X\": X_train_mode, \"y\": y_train_mode},\n",
        "            kwargs={\"random_state\": 42},\n",
        "            outputs=[\"X\", \"y\"]\n",
        "            ),\n",
        "]\n",
        "\n",
        "# 猬锔 upload the mode fill balanced train set\n",
        "#seclea.upload_dataset_split(X=X_train_mode_smote,\n",
        " #                     y=y_train_mode_smote,\n",
        "  #                    dataset_name=\"Heart failure - Mode Fill - Smote Train\",\n",
        "   #                   metadata={},\n",
        "    #                  transformations=mode_smote_transformations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yW-qq4B3pF7a",
        "outputId": "cd68dc03-6fcc-4a01-97fd-ae4c7de0b8fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X before SMOTE: (4347, 20)\n",
            "    Shape of X after SMOTE: (7012, 20)\n",
            "Shape of y before SMOTE: (4347,)\n",
            "    Shape of y after SMOTE: (7012,)\n",
            "Shape of X before SMOTE: (4347, 20)\n",
            "    Shape of X after SMOTE: (7012, 20)\n",
            "Shape of y before SMOTE: (4347,)\n",
            "    Shape of y after SMOTE: (7012,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "classifiers = {\n",
        "    \"RandomForestClassifier\": RandomForestClassifier(),\n",
        "    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n",
        "    \"GradientBoostingClassifier\": GradientBoostingClassifier()\n",
        "}\n",
        "\n",
        "datasets = [\n",
        "    (\"Const Fill\", (X_train_const, X_test_const, y_train_const, y_test_const)),\n",
        "    (\"Mode Fill\", (X_train_mode, X_test_mode, y_train_mode, y_test_mode)),\n",
        "    (\"Const Fill Smote\", (X_train_const_smote, X_test_const, y_train_const_smote, y_test_const)),\n",
        "    (\"Mode Fill Smote\", (X_train_mode_smote, X_test_mode, y_train_mode_smote, y_test_mode))\n",
        "    ]\n",
        "\n",
        "for name, (X_train, X_test, y_train, y_test) in datasets:\n",
        "\n",
        "    for key, classifier in classifiers.items():\n",
        "        # cross validate to get an idea of generalisation.\n",
        "        training_score = cross_val_score(classifier, X_train, y_train, cv=5)\n",
        "\n",
        "        # train on the full training set\n",
        "        classifier.fit(X_train, y_train)\n",
        "\n",
        "        # 猬锔 upload the fully trained model\n",
        "    #    seclea.upload_training_run_split(model=classifier, X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test)\n",
        "\n",
        "        # test accuracy\n",
        "        y_preds = classifier.predict(X_test)\n",
        "        test_score = accuracy_score(y_test, y_preds)\n",
        "        print(f\"Classifier: {classifier.__class__.__name__} has a training score of {round(training_score.mean(), 3) * 100}% accuracy score on {name}\")\n",
        "        print(f\"Classifier: {classifier.__class__.__name__} has a test score of {round(test_score, 3) * 100}% accuracy score on {name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phO-aw27p3xi",
        "outputId": "49f7533f-3404-4934-c4d5-74d55126b30e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classifier: RandomForestClassifier has a training score of 98.3% accuracy score on Const Fill\n",
            "Classifier: RandomForestClassifier has a test score of 97.89999999999999% accuracy score on Const Fill\n",
            "Classifier: DecisionTreeClassifier has a training score of 98.3% accuracy score on Const Fill\n",
            "Classifier: DecisionTreeClassifier has a test score of 97.89999999999999% accuracy score on Const Fill\n",
            "Classifier: GradientBoostingClassifier has a training score of 97.89999999999999% accuracy score on Const Fill\n",
            "Classifier: GradientBoostingClassifier has a test score of 97.6% accuracy score on Const Fill\n",
            "Classifier: RandomForestClassifier has a training score of 98.3% accuracy score on Mode Fill\n",
            "Classifier: RandomForestClassifier has a test score of 97.89999999999999% accuracy score on Mode Fill\n",
            "Classifier: DecisionTreeClassifier has a training score of 98.3% accuracy score on Mode Fill\n",
            "Classifier: DecisionTreeClassifier has a test score of 97.89999999999999% accuracy score on Mode Fill\n",
            "Classifier: GradientBoostingClassifier has a training score of 97.89999999999999% accuracy score on Mode Fill\n",
            "Classifier: GradientBoostingClassifier has a test score of 97.6% accuracy score on Mode Fill\n",
            "Classifier: RandomForestClassifier has a training score of 98.8% accuracy score on Const Fill Smote\n",
            "Classifier: RandomForestClassifier has a test score of 98.1% accuracy score on Const Fill Smote\n",
            "Classifier: DecisionTreeClassifier has a training score of 98.8% accuracy score on Const Fill Smote\n",
            "Classifier: DecisionTreeClassifier has a test score of 98.1% accuracy score on Const Fill Smote\n",
            "Classifier: GradientBoostingClassifier has a training score of 97.7% accuracy score on Const Fill Smote\n",
            "Classifier: GradientBoostingClassifier has a test score of 97.1% accuracy score on Const Fill Smote\n",
            "Classifier: RandomForestClassifier has a training score of 98.8% accuracy score on Mode Fill Smote\n",
            "Classifier: RandomForestClassifier has a test score of 98.1% accuracy score on Mode Fill Smote\n",
            "Classifier: DecisionTreeClassifier has a training score of 98.8% accuracy score on Mode Fill Smote\n",
            "Classifier: DecisionTreeClassifier has a test score of 98.1% accuracy score on Mode Fill Smote\n",
            "Classifier: GradientBoostingClassifier has a training score of 97.7% accuracy score on Mode Fill Smote\n",
            "Classifier: GradientBoostingClassifier has a test score of 97.1% accuracy score on Mode Fill Smote\n"
          ]
        }
      ]
    }
  ]
}